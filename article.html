<!DOCTYPE html> 
<!-- 
    Build upon the website of Sebastian Barfort, 
    https://github.com/sebastianbarfort
-->
<html lang="en-US">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">

<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:wght@500&display=swap" rel="stylesheet"> 
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> 
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/css/bootstrap.min.css" integrity="sha384-GJzZqFGwb1QTTN6wy59ffF1BuGJpLSa9DkKMp0DgiMDm4iYMj70gZWKYbI706tWS" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

<link
  rel="stylesheet"
  href="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css"
/>
<!-- add after bootstrap.min.js -->
<script src="https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
                        TeX: { 
                            equationNumbers: {  autoNumber: "AMS"  },
                            extensions: ["AMSmath.js"]
                        }
});
</script>
<link rel="stylesheet" type="text/css" href="article.css">

<title>RL for DP</title>

<main>
<body data-spy="scroll" data-target="#toc">    
<div class="container">  
    <div class="row">
      <!-- sidebar, which will move to the top on a small screen -->
    <div class="col-sm-3">
      <nav id="toc" data-toggle="toc" class="sticky-top">Table of Contents</nav>
    </div>
        <!-- main content area -->
        <div class="col-sm-6">

<h1> Solving dynamic programming models using methods from reinforcement learning </h1>

<div class="authors">
  <div class="author">Joachim Kahr Rasmussen
    <div class="affil"> CEBI, University of Copenhagen. jkr@econ.ku.dk      
    </div>
  </div>
  <div class="author">Kristian Urup Olesen Larsen
    <div class="affil"> CEBI, University of Copenhagen. kuol@econ.ku.dk      
    </div>
  </div>
</div>

<!--
<p class="author" data-toggle="tooltip" data-placement="right" data-popover="true" 
    data-html=true data-content="<a href='http://www.econ.ku.dk/cebi' target='blank' >CEBI, University of Copenhagen.</a> jkr@econ.ku.dk"> Joachim Kahr Rasmussen</p>
<p class="author" data-toggle="tooltip" data-placement="right" data-popover="true" 
data-html=true data-content="<a href='http://www.econ.ku.dk/cebi' target='blank' >CEBI, University of Copenhagen.</a> kuol@econ.ku.dk"> Kristian Urup Olesen Larsen</p>
-->

<div class="abstract">
  <p>
  In this paper, we assess the applicability of reinforcement learning in approximating solutions to certain types of dynamic stochastic control problems known from the economic literature. In particular, we direct our attention to problems where the state and/or action space is large and potentially continuous - problems that have shown to be challenging to solve with classic approaches from dynamic programming. In order to illustrate how different classes of algorithms know from reinforcement learning can be applied to approximate solutions to dynamic economic models, we first use a simple Q-learner to solve a moderately large cake-eating problem with discrete states and a finite time horizon. We show that for a relatively low approximation error, the problem can be solved fast with reinforcement learning relative to standard backwards induction. Furthermore, we highlight the importance of the initial guess of action-values and the existence of discontinuities for the performance of the algorithm. With these results in hand, we turn our attention to another class of algorithms from reinforcement learning, Deep Q-Networks, that handle models where the state space is continuous. In order to assess the performance of the algorithm, we evaluate it on a set of infinite horizon engine replacement models when the state and action space are first one-dimensional and then multidimensional. Finally, we discuss the applicability of Deep Deterministic Policy Gradients for models where the both states and actions are continuous.
  </p>
</div>

<p>In this section, we go into greater detail with the prospects for using reinforcement learning to augment more traditional techniques from dynamic programming toolbox in solving stochastic control problems in discrete time. However, before we do so, it is instructive to show that - despite a different terminology - the fundamental idea of reinforcement learning is to solve a problem much similar to those that we typically consider in economic dynamic programming models.</p>

<h2>Stochastic Control Problems and Terminology</h2>
<p>In economic models of dynamic programming, we usually use a value function (or a Bellman Optimality Equation) in order to characterize the solution to Markovian Decision Process (MDP, henceforth) - a class of stochastic control problems in discrete time where an agent optimizes an objective function subject to uncertainty, and outcomes are only in part determined by the agent.<label class="sidenote-counter"></label> <span class="sidenote">An underlying assumption for MDP's is that the state captures all information about the past economy that is relevant for the future - this is the so-called Markov property <span class="citation" data-toggle="tooltip" data-placement="bottom" data-popover="true" 
    data-html=true data-content="Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. pages 53-82. A Bradford Book, The MIT Press, 2014. <a href='https://doi.org/10.1007/s10479-012-1077-6' target='blank' >https://doi.org/10.1007/s10479-012-1077-6</a>">(Sutton et al, 2014)</span>
</span>
</p>

<p>With reference to  <span class="citation" data-toggle="tooltip" data-placement="bottom" data-popover="true" 
    data-html=true data-content="Fedor Iskhakov, Thomas H. JÃ¸rgensen, John Rust, and Bertel Schjerning. The endogenous grid methodfor discrete-continuous dynamic choice models with (or without) taste shocks. Quantitative Economics, 8(2):317-365, 2017. doi:10.3982/QE643. <a href='https://onlinelibrary.wiley.com/doi/abs/10.3982/QE643' target='blank' >https://onlinelibrary.wiley.com/doi/abs/10.3982/QE643</a>">(Ishakov et al, 2017)</span>, we may summarize a stylized version of an MDP in Dynamic Programming in the following way:</p>
<ul>
    <li>At any time $t\in\{0,1,...,T\}$, an agent observes (part of) the state of the economy, $s\in\mathcal{S}$, where $s$ is potentially high-dimensional such that $\mathcal{S}\subseteq\mathbb{R}^m$.</li>
    <li>Based on this observation, the agent chooses a decision or action, $a\in\mathcal{A}\left(s\right)$, where $a$ is also potentially high-dimensional such that $\mathcal{A}\left(s\right)\subseteq\mathbb{R}^n$.</li>
    <li>When an action is made, the agent earns instantaneous utility, $u\left(a_t,s_t\right)$, where $u: \mathcal{A}\left(s\right)\times \mathcal{S}\rightarrow \mathbb{R}$.</li>
    <li>Eventually, the state for period $t+1$, $s_{t+1}$, is drawn with transition probability $p\left(s_{t+1}\mid s_t,a_t\right)$.</li>
</ul>
<p>Note that if $T$ is finite, we can in principle think of $t$ as being part of the state, where it evolves deterministically between periods.</p>

<p>The objective of the agent is to choose a sequence of actions subject to the economic environment (i.e. the transition of states) such that expected intertemporal (discounted) utility</p>
\begin{equation}\label{utility1}
    U:=E_0\left[\sum_{t=0}^T\beta^t u\left(a_t,s_t\right)\right]
\end{equation}
<p>is maximized subject to a set of constraints. Here, the discount factor, $\beta$, satisfies $\beta\in\left(0,1\right)$. The (optimal) value function at any point in time is now simply an expression for the expected discounted utility conditional on optimal behavior:</p>

\begin{align}\label{value101}
V\left(s_{t}\right) &=\underset{\Pi_t}{\text{sup}}\text{ }E\left[\sum_{k=0}^{T}\beta^{k}u\left(a_{t+k},s_{t+k}\right)|s_{t},a_{t}\right] \\ &=\underset{a_{t}\in\mathcal{A}\left(s_{t}\right)}{\text{max}}\text{ }u\left(a_{t},s_{t}\right)+\beta\int V\left(s_{t+1}\right)p\left(s_{t+1}\mid a_{t},s_{t}\right)\text{d}s_{t+1}
\end{align}

<p>where $\Pi_t=(a_t,a_{t+1},...)$ is a sequence of actions satisfying the constraints of the model. When the time horizon is infinite, this can be written in recursive form as a Bellman equation. Equivalently, we may define the action-value function as the value of choosing some (potentially suboptimal) decision $a_t$ conditional on future optimal behavior:</p> 

\begin{equation}\label{Q1}
    Q\left(a_t,s_t\right)=u\left(a_t,s_t\right)+\beta\int V\left(s_{t+1}\right)p\left(s_{t+1}\mid a_{t},s_{t}\right)\text{d}s_{t+1}
\end{equation}

<p>This 'type' of value-function, however, is more frequently used in the RL-literature than what is the case in the DP literature. In the next sections, it will be clearer why.</p>
 
<p>In turn, a prominent aim in dynamic programming problems is to characterize the policy function $a_t^*\left(s_t\right)$ that characterizes the optimal set of actions conditional on the state - that is, the action $a_t\in\mathcal{A}\left(s_t\right)$ that is compatible with \eqref{value101}. We may write this as: 
\begin{equation}
a_t^*(s_t):=\underset{a_t\in\mathcal{A}\left(s_t\right)}{\text{arg max}}\text{ }Q\left(s_t,a_t\right)
\end{equation}
Similar to economic models of dynamic programming, the purpose of an agent within the reinforcement learning-framework is to choose actions that are compatible with the constraints of some dynamic model in order to maximize some objective. Yet, both the terminology and the timing of the frameworks are somewhat different.  <span class="citation" data-toggle="tooltip" data-placement="bottom" data-popover="true" 
data-html=true data-content="Richard S. Sutton and Andrew G. Barto. Reinforcement learning: An introduction. pages 53-82. A Bradford Book, The MIT Press, 2014. <a href='https://doi.org/10.1007/s10479-012-1077-6' target='blank' >https://doi.org/10.1007/s10479-012-1077-6</a>">Sutton et al, (2014)</span> refers to MDP's as a general framework for studying a wide range of problems that have been subject to RL-algorithms. Their description generally follows the structure above, but their terminology differs slightly.</p> 

<p>In particular, within the RL-literature, the objective function of the problem - what is often modeled as intertemporal utility in economic models - is generally referred to as a goal, $G$, while the instantaneous utility is instead referred to as a reward, $r$. While economic DP models often model utility as some (likely smooth and monotone) function of an action (e.g. consumption) and potentially a state (e.g. age, retirement or children), a reward is a similar concept that reflects the success of an agent (e.g. winning a video game, opening a door with a robot arm, etc.) that generally needs no smoothness. Equivalently, while discounting in economic MDP's that are subject to DP reflects the time-preferences of an economic agent, discounting in classic MDP's subject to RL is a slightly broader concept reflecting e.g. the relative value of winning a video game faster (or losing in more time) or opening a door with a robot arm in fewer attempts.</p>
 
<p>Finally, the RL-literature typically refers to the environment, $E$, as model structure that stochastically maps per-period states and actions into next-period states and - potentially - rewards if the reward is stochastic. This concept is effectively not different from the set of constraints and the transition probability functions that are considered to drive state-transitions in dynamic programming.</p>

<h2>Solving Stochastic Control Problems with Dynamic Programming</h2>
<p>In stochastic control problems where the state and action space are both discrete, value functions are elements in a subset of a finite-dimensional Euclidean space, $\mathbb{R}^{|S|}$. In these cases, MDP's can be solved exactly with dynamic programming, implying that the value function and the associated optimal decision rule (the policy rule) can be extracted without approximation error \citep{Rust_1996} by using some variation of value function iteration or policy iteration approaches as introduced by \citet{Howard_1960}. When the horizon is finite, value function iteration is essentially analogous to backwards induction that simply solves for the optimal value function and extracts the associated optimal policy.  When instead the horizon is infinite, value function iteration relies on the so-called Bellman-operator to be a contraction mapping with a fixed point $V^*$ for convergence from any starting point $V_0$ through successive approximations. Finally, policy iteration involves an additional step of solving for the optimal policy conditional on the iterated value function \citep{sargent_2020}. The latter approach has shown to be faster in many applications - in particular when the time-preference rate is close to one \citep{judd_2008}.</p>
 
<p>However, when the state space of an MDP is not discrete, and the action space is either discrete (DDP's) or continuous (CDP's), these standard approaches are not directly implementable. Many economic models of state dynamics in e.g. wealth or technology can be characterized by such setup. When MDP's are continuous, value functions are instead bounded and real-valued functions on the state space $S$, implying that the optimal decision rule and value function must be approximated with either discrete or smooth approximation methods \citep{Rust_1996}.</p>

<p>Both of these approaches for approximation fundamentally require that the state space is initially discretized into $S=\{s_1,...,s_N\}$, where $N$ is a finite number \citep{Rust_1996}. If the state space were initially unbounded, this also implicitly implies that some bounding of the state space must take place. Furthermore, the transition density function, $p\left(s'\mid a,s\right)$ must be transformed into a discrete probability function, $p_N\left(s'\mid a,s\right)$, in order to evaluate the expected future (random) value function. There exists a large literature on both efficient discretization of the state space and the numerical integration that is associated with calculating the expected value function from a transition density function.</p>
 
<p>A particular concern for specifying a grid is the curse of dimensionality. Even when the state space is lower dimensional, the researcher faces trade-off when specifying the size of a grid, $N$. On the one hand, choosing a high $N$ generally implies that the approximation error of the true value function will be smaller. However, in many practical applications, a high $N$ also implies that each iteration in the algorithm requires more computational power. As the dimensionality of the state space increases, the latter concern generally blows up, and the MDP quickly becomes computationally exhaustive to solve. Referring to \citet{Rust_1997_no2}, we may say that a problem suffers from the curse of dimensionality when the computational time/power needed to achieve a given precision in an approximation of a solution to an MDP increases exponentially in the dimensionality of the problem \footnote{Or slightly more formally, a problem suffers from the curse of dimensionality when the (worst case) complexity of the problem has an upper bound that grows more than polynomially in the dimensionality of the state (and action) space \citep{Rust_1997_no2}}. Among traditional grid-methods, neither uniform grids nor quadriture grids have shown to circumvent the curse of dimensionality in general (at least on a worst case basis), while so-called "low discrepancy"-grids and random grids have generally shown more promise in circumventing the curse of dimensionality associated with multivariate integration on an average basis and a worst case basis, respectively \citep{Rust_1996}\footnote{According to \citep{Rust_1997_no2}, we may think of there as being two potential curses of dimensionality - one of multidimensional numerical integretaion and one of function approximation.}. On top of these methods, a large range of (more or less model-specific) approaches have been developed to choose grid-points for both states and policies in a manner that reduces computational power and/or approximation errors\footnote{See for instance the endogenous grid-method for policies (e.g. used by \citet{Iskhakov_2017} and discussed in this course) for DDP's and the recent literature on sparse grid methods as discussed by \citet{Iskhakov_2020}.}.</p>
 
<p>Discrete approximation methods generally rely on solving the continuous MDP as a "nearby" discrete problem or relying on a "self-approximating" property of the so-called random Bellman-operator in order to evaluate the value function at any $s$ in $S$ as described by \citet{Rust_1997_no2}. In contrast to discrete approximation methods, smooth approximation methods rely on interpolation between grid points of the optimal value function (or in principle the policy function) in order to obtain approximations at any arbitrary point, $s$, in the true state space $S$ \citep{cai_2015}. In essence, interpolation relies on using approximations of the value function in some grid-point, $s_i$, as an estimate of the true value function that can be used to approximate the value function outside the grid. If the target function is known to be sufficiently smooth, or if the grid points of the state space are selected efficiently such that they are concentrated in areas with more curvature or kinks (such as the adaptive sparse grid algorithm suggest by \citet{Brumm_2017}), then a relatively low amount of grid-points, $N$, can be selected at a small approximation cost. Among traditional methods are piecewise linear and cubic interpolation, Chebyshev polynomials and actual regression-based interpolation. While none of these methods eventually do not circumvent the curse of dimensionality, they may reduce the computational power required to undertake large-dimensional function approximations significantly \citep{Rust_1996}.</p>

<h2>Solving Stochastic Control Problems with Reinforcement Learning</h2>
<p>The main insight of RL is that dynamic problems can be approximately solved by solving respectively a prediction problem for the value function, and a control problem for the policy function \citep{Li_2017}. As explained by \citet{Iskhakov_2020} and outlined previously in this section, reinforcement learning has essentially adopted the problem structure from dynamic programming models and introduced so-called asynchronous stochastic approximation algorithms; algorithms that update action-values at partially random states and actions using whatever information about other state-specific values that is available at the given point in time. Such updating step can - in a simple set of algorithms - be represented by the following line: $$Q\left(s,a\right)\leftarrow\eta Q\left(s,a\right)+\left(1-\eta\right)\left(r+\gamma V\left(s'\right)\right)$$
where $s'$ is the realized state from taking action $a$ in state $s$, $r$ is the reward (equivalent to instantaneous utility) from taking the action, and $\gamma$ is equivalent to a discount factor. Such approach for updating the value functions contrasts most traditional DP-algorithms that involve (computationally costly) operations over the full state-space of the MDP \citep{sutton_2014}. This intuitively makes RL-algorithms well-suited for approximating discrete problems where the state-space is very large (e.g. in games such as chess, go and backgammon), while slow for small problems where no approximation is necessary given that each action-value in the state space must generally be visited several times before exact convergence.</p>
 
<p>Thinking about how asynchronous stochastic updating of action-values works more broadly in reinforcement learning, then unlike methods from dynamic programming, we may say that RL-methods are unified through their environment-model interface. Action-values in RL models are estimated or \textit{trained} via sequences of interactions with the environment. On an intuitive level this works very much in the same way that humans learn from experience - the model produces a policy $a$ which yields ``environment feedback'' $E(a)$ and utility $r$ which can jointly be used to improve the model. The environment itself can be thought of as a collection of (probabilistic) equations or decision rules that specify how actions translate into rewards and state-changes.</p>
 
<p>A convenient feature of RL-algorithms is that the transition density, $p\left(s'\mid s,a\right)$ need not be identified. This insight comes from observing that the action-specific value function, $Q\left(s,a\right)$, is an implicit function of the transition density. As long as one can draw simulated observations from $p\left(s'\mid s,a\right)$ and train the algorithm, one can also learn the action-value function, $Q\left(s,a\right)$. This is in essence what makes reinforcement learning \textit{model-free} \citep{Iskhakov_2020}. While this is a significant advantage of RL-algorithms in situations where learning takes place 'live' from real data, we will fully use of this property in this paper.</p>
 
<p>Q-learning is a simple, yet prominent, example of an RL-algorithm for problems with a finite state space - and generally a finite time horizon \citep{Iskhakov_2020} - that relies on the exact idea of randomly visiting different elements in the state-space and asynchronously updating associated action-values as described before. This algorithm has shown to converge to optimum action-values (or a fixed point) with probability one if (i) all action-states are repeatedly sampled with positive probability, and (ii) the action-space is discrete \citep{Watkins_1992}. We go more into depth with this algorithm in section \ref{section_introductory_example}.</p>
 
<p>Similar to dynamic programming, there is a large literature within reinforcement learning that is concerned with function approximation. While the simple Q-learner only solves problems with a finite set of states and actions, function approximation allows RL-algorithms to approximate action-values when states and/or actions are continuous and - potentially - unbounded. Deep RL leverages the universal approximation property of neural networks to theoretically generate arbitrarily complex inter- and extrapolations from a set of realized state transitions $s,a,s'$ via $p(\cdot)$. In practice training a neural network to represent $Q$ is not straight forward for multiple reasons. Most importantly a good approximation of $Q$ must be consistent with $Q(s, a)\approx r + \beta \max_{a'} Q(s', a')$. $Q$ is present on both sides of the equality, and so finding a suitable set of neural network weights will be a case of \textit{``chasing ones own tail''}.</p>
 
<p>\citet{mnih_human-level_2015} present several methods for mitigating this issue in practice, which we debate in greater detail in section \ref{seq:DQN}. Because the data used to train deep RL models are simulated directly from $p(s'|s,a)$ with $a$ chosen by model, the network will generally improve its representation of $Q$ in regions of state and action space which lie along the apparent optimal path. Extrapolations can be wildly inaccurate, but will improve for all relevant $s,a$-subspaces as more data are generated and revealed to the model. This gives RL models the attractive feature of being able to leverage the flexibility of a deep neural network for approximating $Q$ in areas of the problem space $\mathcal{S}\times \mathcal{A}$ where an approximately optimally behaving agent might find itself, and at the same time leaves the researcher with typically poor out-of-sample predictions when evaluating values far from the equilibrium path.</p>
 
<p>RL models also suffer in some particular settings where tailored methods from DP prevail. When the future is very mildly discounted it becomes difficult to assign a value to it via a neural network because the training relies on the stability of the neural network representation of the value function even when training is incomplete. Thus RL models seem to work best with problems that feature modest discount rates or shut down the potential for infinite nonzero instantaneous utility in their structure. Some problem structures, including the stopping problem of Harold Zurcher also turn out to lend themselves poorly to RL because they feature large areas of the state space which are entirely irrelevant for learning about the optimal policy. Yet, as we show in this paper, RL model can indeed solve these models with low approximation error.</p>

<p>In particular, we find that deep RL models have both advantages and disadvantages over the traditional methods from dynamic programming. By using a neural network to approximate the value function, interpolation becomes an intrinsic part of the estimation procedure, and attention is automatically given to those areas of state space where realizations are likely to be found. Compared to DP, this takes some control out of the researchers hands - arguably a beneficial consequence since the model will itself take care of interpolation where necessary, and ignore it where unnecessary. However, we also find that the potentially significant approximation error that can arise due to insufficient training or poor choices of model architecture is consequence of the approach that will in some cases render RL inappropriate as a tool. Whether this error exceeds the one derived from interpolation between grid points will be case specific.</p>
</div>
<div class="col-sm-3"></div>
</div>
</div>

</main>
</body>

<script>
  var originalLeave = $.fn.popover.Constructor.prototype.leave;
  $.fn.popover.Constructor.prototype.leave = function(obj){
    var self = obj instanceof this.constructor ?
      obj : $(obj.currentTarget)[this.type](this.getDelegateOptions()).data('bs.' + this.type)
    var container, timeout;
  
    originalLeave.call(this, obj);
  
    if(obj.currentTarget) {
      container = $(obj.currentTarget).siblings('.popover')
      timeout = self.timeout;
      container.one('mouseenter', function(){
        clearTimeout(timeout);
        container.one('mouseleave', function(){
          $.fn.popover.Constructor.prototype.leave.call(self, self);
        });
      })
    }
  };
  
  $(document).popover({ selector: '[data-popover]', trigger: 'hover click', delay: {show: 30, hide: 400}});
  </script>
  
</html>
